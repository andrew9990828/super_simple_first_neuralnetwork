# Neural Network from Scratch

My first neural network implementation built entirely from scratch in NumPy â€” no frameworks, just fundamentals.

## ğŸ¯ What It Does

Trains a single-layer network to predict the sum of 3D vector components. Given inputs like `[2, 3, 4]`, the network learns to output `9`.

## ğŸ”„ The Training Loop

1. **Forward Pass**: Multiply inputs by weights, add bias (`Wx + b`)
2. **Loss Computation**: Calculate mean squared error
3. **Optimization**: Nudge each weight up/down, keep changes that reduce loss
4. **Repeat**: Train over multiple epochs

## ğŸ§  Key Concepts Learned

- Matrix multiplication for batched operations
- Forward propagation through linear layers
- Loss computation (MSE)
- Basic gradient descent (numerical approximation)
- Weight vs bias roles

## ğŸ“Š Architecture

```
Input: (100, 3)  â†’  Weights: (3, 1)  â†’  Output: (100, 1)
                        â†“
                    Bias: (1,)
```

**Training Data**: 100 random 3D vectors  
**Target**: Sum of each vector's components  
**Parameters**: 3 weights + 1 bias

## ğŸš€ Usage

```bash
python neural_network_one.py
```

Expected output:
```
Epoch 0 | Loss: ...
[[w_x w_y w_z]]
Epoch 1 | Loss: ...
[[w_x w_y w_z]]
...
```

## ğŸ’¡ Why This Matters

Every complex neural network â€” from GPT to diffusion models â€” follows this same training loop. This implementation strips away abstractions to reveal the mathematical core.

## ğŸ“ Notes

- **Status**: Intentionally minimal
- **Next Steps**: Multiple layers, activation functions, analytical gradients
- **Purpose**: Build concrete understanding, one operation at a time

## ğŸ“„ License

MIT

---

*First step in understanding how neural networks actually work under the hood.*